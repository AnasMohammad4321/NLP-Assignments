{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99rVwJ3tVJLc",
        "outputId": "125a3e88-fe20-4b32-f32e-302303dff74a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UrduSentimentDataset:\n",
        "    def __init__(self, csv_file, max_vocab_size=None):\n",
        "        self.df = pd.read_csv(csv_file, delimiter='\\t')\n",
        "        self.df['Class'] = self.df['Class'].map({'P': 1, 'N': 0})\n",
        "        self.tokenize_and_pad(max_vocab_size)\n",
        "\n",
        "    def tokenize_and_pad(self, max_vocab_size):\n",
        "        all_text = ' '.join(self.df['Tweet'])\n",
        "        words = all_text.split()\n",
        "        word_counts = Counter(words)\n",
        "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "        if max_vocab_size is not None:\n",
        "            sorted_vocab = sorted_vocab[:max_vocab_size]\n",
        "        self.int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "        self.vocab_to_int = {w: k for k, w in self.int_to_vocab.items()}\n",
        "\n",
        "        self.encoded_tweets = [[self.vocab_to_int.get(word, 0) for word in tweet.split()] for tweet in self.df['Tweet']]\n",
        "\n",
        "        max_len = max([len(tweet) for tweet in self.encoded_tweets])\n",
        "        self.padded_tweets = np.array([tweet + [0]*(max_len-len(tweet)) for tweet in self.encoded_tweets])\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.padded_tweets, self.df['Class'], self.vocab_to_int\n",
        "\n",
        "    def print_samples(self, num_samples=5):\n",
        "        print(\"Random samples from the dataset:\")\n",
        "        samples_indices = np.random.choice(len(self.df), num_samples, replace=False)\n",
        "        for idx in samples_indices:\n",
        "            tweet = self.df.loc[idx, 'Tweet']\n",
        "            label = self.df.loc[idx, 'Class']\n",
        "            print(f\"Tweet: {tweet} | Label: {'Positive' if label == 1 else 'Negative'}\")\n",
        "\n",
        "    def preprocess_data(self, test_size=0.25, random_state=42):\n",
        "        X, y = self.padded_tweets, self.df['Class']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        X_train_clean = X_train.copy()\n",
        "        y_train_clean = y_train.copy()\n",
        "        X_test_clean = X_test.copy()\n",
        "        y_test_clean = y_test.copy()\n",
        "\n",
        "        nan_mask_train = ~np.isnan(y_train_clean)\n",
        "        X_train_clean = X_train_clean[nan_mask_train]\n",
        "        y_train_clean = y_train_clean[nan_mask_train]\n",
        "\n",
        "        nan_mask_test = ~np.isnan(y_test_clean)\n",
        "        X_test_clean = X_test_clean[nan_mask_test]\n",
        "        y_test_clean = y_test_clean[nan_mask_test]\n",
        "\n",
        "        y_train_clean = y_train_clean.astype(int)\n",
        "        y_test_clean = y_test_clean.astype(int)\n",
        "\n",
        "        y_train_clean = to_categorical(y_train_clean, num_classes=2)\n",
        "        y_test_clean = to_categorical(y_test_clean, num_classes=2)\n",
        "\n",
        "        return X_train_clean, X_test_clean, y_train_clean, y_test_clean"
      ],
      "metadata": {
        "id": "52cWwBmYWD4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://raw.githubusercontent.com/MuhammadYaseenKhan/Urdu-Sentiment-Corpus/master/urdu-sentiment-corpus-v1.tsv'\n",
        "dataset = pd.read_csv(URL, delimiter='\\t')\n",
        "dataset.to_csv('urdu-sentiment-corpus-v1.tsv', sep='\\t', index=False)\n",
        "\n",
        "dataset = UrduSentimentDataset('urdu-sentiment-corpus-v1.tsv')\n",
        "dataset.print_samples(num_samples=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLCgBayGWiQD",
        "outputId": "0e288ec4-d958-4f01-c3b0-54872e557634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random samples from the dataset:\n",
            "Tweet:  بھائ نہ جھک مارو نہ حق بس تمیز سے زندگی گزارو | Label: Positive\n",
            "Tweet: نئے بجلی میٹرز کے حوالے سے خبریں بے بنیاد : وزیر اعظم معائنہ کمیشن  | Label: Negative\n",
            "Tweet: سیکیورٹی خدشہ،عمران خان کے کنٹینر کے پیچھے 2 کنٹینر رکھ دیئے  | Label: Positive\n",
            "Tweet:  خدانخواسته عمران خان تیرے گھر گھس گیا تو کیا هو گا اس کی تصویر بھی لگا دو | Label: Negative\n",
            "Tweet: دوسرے اضلاع سے آنے والے ٹریفک وارڈنز اور افسران اپنے وائرلیس سیٹ اور گاڑیاں بھی ساتھ لائیں گے ۔  | Label: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# Load your Urdu dataset\n",
        "dataset = pd.read_csv('urdu-sentiment-corpus-v1.tsv', delimiter='\\t')\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_tweets = [word_tokenize(tweet) for tweet in dataset['Tweet']]\n",
        "\n",
        "# Train Word2Vec embeddings\n",
        "embedding_dim = 100  # You can adjust the embedding dimension as needed\n",
        "word2vec_model = Word2Vec(sentences=tokenized_tweets, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the trained embeddings to disk\n",
        "word2vec_model.save(\"word2vec_urdu_embeddings.model\")\n"
      ],
      "metadata": {
        "id": "z3IUkNxJWoYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the pre-trained embeddings\n",
        "word2vec_model = Word2Vec.load(\"word2vec_urdu_embeddings.model\")\n",
        "\n",
        "# Get the embedding vector for a specific word\n",
        "word_vector = word2vec_model.wv['سلام']  # Replace 'سلام' with the desired word"
      ],
      "metadata": {
        "id": "udGFXaHdYwOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall glove_python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfP9SWTn7VZ2",
        "outputId": "4c672e1d-53b9-4d70-a56c-cdb713e1cc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove_python\n",
            "  Using cached glove_python-0.1.0.tar.gz (263 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy (from glove_python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from glove_python)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: glove_python\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for glove_python (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for glove_python\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for glove_python\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for glove_python\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build glove_python\n",
            "\u001b[31mERROR: Could not build wheels for glove_python, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv-RO00-XYvG",
        "outputId": "83a45496-4091-4b62-8a30-9b7045430293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# Load your Urdu dataset\n",
        "dataset = pd.read_csv('urdu-sentiment-corpus-v1.tsv', delimiter='\\t')\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_tweets = [word_tokenize(tweet) for tweet in dataset['Tweet']]\n",
        "\n",
        "# Save tokenized tweets to a text file\n",
        "with open('tokenized_tweets.txt', 'w', encoding='utf-8') as f:\n",
        "    for tweet_tokens in tokenized_tweets:\n",
        "        tweet_str = ' '.join(tweet_tokens)\n",
        "        f.write(tweet_str + '\\n')\n"
      ],
      "metadata": {
        "id": "QjRVLT8e8FRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "# Train GloVe embeddings\n",
        "corpus = Corpus()\n",
        "corpus.fit(tokenized_tweets, window=5)\n",
        "glove_model = Glove(no_components=embedding_dim, learning_rate=0.05)\n",
        "glove_model.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "glove_model.add_dictionary(corpus.dictionary)\n",
        "\n",
        "# Save the trained embeddings to disk\n",
        "glove_model.save(\"glove_urdu_embeddings.model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "m9aOeGD1XSGm",
        "outputId": "68ee4d92-c740-42ac-c73e-e946e3d72231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'glove'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ecd9db54fbdf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train GloVe embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glove'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfiXHo-HX47C",
        "outputId": "2398f921-bf06-4e47-bac6-2e5f873b1f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=cecd59aa62eb7ff38e174f5a3fa9fa02e717fa9ed781ca88c1da3d508bcaf84c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Train FastText embeddings\n",
        "model = fasttext.train_unsupervised('urdu-sentiment-corpus-v1.tsv', model='skipgram', dim=embedding_dim)\n",
        "\n",
        "# Save the trained embeddings to disk\n",
        "model.save_model(\"fasttext_urdu_embeddings.bin\")\n"
      ],
      "metadata": {
        "id": "ANq5yCE2V4Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenized tweets to a text file\n",
        "with open(\"urdu_tweets.txt\", \"w\") as f:\n",
        "    for tweet in tokenized_tweets:\n",
        "        f.write(\" \".join(tweet) + \"\\n\")\n",
        "\n",
        "# Train FastText embeddings\n",
        "import fasttext\n",
        "fasttext_model = fasttext.train_unsupervised(\"urdu_tweets.txt\", model='skipgram', dim=embedding_dim)\n",
        "\n",
        "# Save the trained embeddings to disk\n",
        "fasttext_model.save_model(\"fasttext_urdu_embeddings.bin\")\n"
      ],
      "metadata": {
        "id": "NAl2dJ88YXAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Load the trained FastText model\n",
        "fasttext_model = fasttext.load_model(\"fasttext_urdu_embeddings.bin\")\n",
        "\n",
        "# Get the embedding vector for a specific word\n",
        "word = \"سلام\" # Replace with the desired Urdu word\n",
        "word_vector = fasttext_model.get_word_vector(word)\n",
        "\n",
        "# Print the word and its vector\n",
        "print(f\"Word: {word}\")\n",
        "print(f\"Vector: {word_vector}\")"
      ],
      "metadata": {
        "id": "bZelsghZY-IK",
        "outputId": "f9c2643b-631f-40ff-cd30-faabe3ccd9b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: سلام\n",
            "Vector: [ 0.08545976  0.03895957  0.05232018 -0.01832284  0.21562408  0.07658491\n",
            " -0.16425034  0.02151671  0.12563014  0.24210063 -0.12327452 -0.04418094\n",
            " -0.03638594  0.05983236  0.05899256  0.07088875  0.05895574  0.07567459\n",
            " -0.01947038 -0.11628506  0.00513881 -0.02856266  0.0447617  -0.07029486\n",
            " -0.02756987 -0.07749857  0.04043348 -0.122174   -0.09659746  0.26377687\n",
            " -0.06651026 -0.05298331 -0.18661416 -0.00845781 -0.23886631 -0.07387757\n",
            "  0.05268278 -0.16007702 -0.2942576   0.00972584 -0.05140448  0.02618704\n",
            " -0.04078184  0.07400005  0.19353682 -0.15319799  0.00567065 -0.06872339\n",
            " -0.07694431  0.11209474 -0.16405527 -0.00406146 -0.22993062  0.15368865\n",
            " -0.02056284 -0.12981896  0.03362524  0.05526285  0.05316765  0.11128714\n",
            "  0.04779462 -0.06031324  0.26931387  0.11814883 -0.04205162  0.06585701\n",
            "  0.03494747  0.19793032  0.10714965 -0.00047629 -0.28482154  0.13684209\n",
            "  0.06011401  0.10452374 -0.08700595 -0.05810168  0.13502426  0.07547034\n",
            "  0.23994885 -0.01147719  0.07312121 -0.00235626  0.04659772 -0.24098964\n",
            " -0.0195245   0.01586234  0.09183847  0.13233021 -0.1245399  -0.13154869\n",
            "  0.24647598 -0.04497242  0.3743239  -0.28067777 -0.117046   -0.28910056\n",
            " -0.12158563  0.09463912 -0.03357495 -0.02426616]\n"
          ]
        }
      ]
    }
  ]
}